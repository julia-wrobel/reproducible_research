---
title: "Motivation for Reproducible Research"
author: "Hitchhiker's Guide to Reproducible Research <br> <span style = 'font-size: 75%;'> Julia Wrobel and David Benkeser  </span>"
date: '<br> `r icons::icon_style(icons::fontawesome("link"), fill = '#ffffff')` [.white[Course Website]](https://bit.ly/sismid_hitchhikers)'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      titleSlideClass: ["left", "middle", "inverse"]
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
# define a class for a block of text with mono spaced print
extra_css <- list(
  ".monobox" = list(
    display = "inline-block", 
    width = "90%",
    height = "400px",
    padding = "5px",
    border = "1px solid #012169",
    `background-color` = "#d9d9d6",
    `font-family` = "Courier Prime",
    `align-items` = "left",
    `vertical-align` = "middle"
  ),
  ".small" =  list(`font-size` = "80%"),
 	".red" = list(color = "#da291c"),
  "ul li" = list(`margin-bottom` = "10px"),
  "a" = list(color = "#007dba"),
  "a:hover" = list("text-decoration" = "underline")
)

style_mono_accent(
  base_color = "#012169",
  header_font_google = google_font("DM Sans"),
  text_font_google   = google_font("DM Sans", "400", "400i"),
  code_font_google   = google_font("Courier Prime"),
  extra_css = extra_css
)
```

<style type="text/css">
.remark-slide-content {
    font-size: 22px
}
</style>

## An email you may have received

.center[
	.monobox[
		.left[
			David, 
			<br>
			<br> After review, we found that some data may have been 
			mislabeled. Could you re-run the analyses and update numbers in 
			Tables 1-3 and re-create Figure 2?
			<br>
			<br> We would like to submit this paper by the end of the week.
			<br>
			<br> Best wishes, 
			<br> Your favorite collaborator
		]
	]
]

???

Very commonly in collaborations, the data change. It could be 
due to errors, more data being added, etc... The goal of this class 
is to make it so you do not fill with dread when you receive such an 
e-mail.

We are working toward a world where you download the new data,
type one command and voila! No by-hand entering numbers into tables. 
No copy-pasting from R/SAS/STATA to Word. Just one click.

---

## An email you may have received

.center[
	.monobox[
		.left[
			David, 
			<br>
			<br> The data will be finalized tomorrow and we need to have 
			new Tables to the data safety monitoring board in two days. Hope
			you did not have plans this weekend!
			<br>
			<br> Respectfully, 
			<br> Your boss
		]
	]
]


???


To prepare for situations like this, project organization is key. We will discuss strategies for preparing fast-turnaround projects. But just because we are turning projects around quickly, does not mean we are not working carefully. By having a set, reproducible workflow in place before data acquisition, we can have the best of both worlds. Careful analysis planning and execution AND fast delivery.

---

## An email you may have received

.center[
	.monobox[
		.left[
			David, 
			<br>
			<br> IT just updated R to the latest version on my computer and my analysis code no longer runs. I get strange error messages about compilers and package versions. My defense is next week. I am starting to panic.
			<br>
			<br> Most sincerely,
			<br> A frantic PhD student
		]
	]
]

???

Maintaining stable code can be challenging! We'll discuss the tools that make this easier to achieve. 

---

## A personal story

In Fall 2020, I became involved in the five US government-funded studies of COVID-19 vaccines.

__Scientific goal__: understand whether/how immune responses do/not correlate with risk of COVID-19.
* Referred to as vaccine *correlates of protection*
* Aids regulatory decisions, vaccine development, motivates updated vaccine formulations, ...

Needed a harmonized analysis across all five trials. And fast!

.red[No direct access to Moderna's data!]

---

## Reproducibility


```{r repro-fig, echo = FALSE, message = FALSE, warning = FALSE, fig.height=4, dpi=300}
source("repro_fig.R")
my_sci_figure(exps_diffs)
```
.center[
	.small[
		Working terminology (figure produced using [scifigure](https://cran.r-project.org/web/packages/scifigure/vignettes/Visualizing_Scientific_Replication.html) R package)
	]
]

???

We will focus on building analyses that satisfy the reproducibility 
definition outlined above. Could a different analyst sit down with your code, 
execute it and get exactly the same results?

Reproducibility is a *minimal* standard. Just because something is 
*reproducible* does not necessarily imply that it is *correct*. The 
code may have bugs. The methods may be poorly behaved. But reproducibility 
is likely correlated with correctness -- if you are careful enough to 
make everything reproducible, you're probably more likely to be careful
in designing the analysis in the first place. 

We will use replicability to mean a new experiment using the same methods
coming to the same conclusions. 

Generalizability refers to taking conclusions from your study population
and generalizing them to a new population. 

---

## Other definitions of reproducibility

Our working definition of reproducibility: 

.center[.red[Could a different analyst sit down with your code, 
execute it (with minimal effort?) and get exactly the same results?]]

Many perspectives on reproducibility/replicability/etc... Some readings:
- [Peng *Reproducible Research in Computational Science*](https://www.science.org/doi/10.1126/science.1213847)
- [Patil, et al. *A statistical definition of reproducibility*](https://www.biorxiv.org/content/biorxiv/early/2016/07/29/066803.full.pdf)
- [Goodman, et al. *What does research reproducibility mean?*](https://www.science.org/doi/10.1126/scitranslmed.aaf5027)

---
class: title-slide, center, inverse, middle
background-color: #84754e

*You mostly collaborate with yourself, and me-from-two-months-ago never responds to email.*
<div> - Karen Cranston

---

## Why should I care?

* Careful coding means more likely to produce __correct results__.
* In the long run, you will __save (a lot of) time__.
* Higher __impact__ of your science.
* __Avoid embarrassment__ on a public stage. 

???

Doing reproducible science is time consuming, but is ultimately time-saving.
As you practice these tools more and more, they will start to become
habits. I do analyses now in a day or so (and at much higher quality) now
that would take me a (frustrating) week seven or eight years ago. 

One of my great fears as an academic is being accused of intentionally 
doing bad science. Leaving a paper trail that any one else can follow is
a bit intimidating -- we all make mistakes. Reproducible work indicates 
that any mistakes made were likely honest.


---

## Odyssey of Baggerly and Coombes

A prominent [reproducibility horror story](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3474449/) occurred in the late 20-00's at Duke University Institute for Genome Sciences and Policy.

Team was using __genomic signatures__ to predict the __most effective chemo regimen__ for cancer patients.

Keith Baggerly and Kevin Coombes (MD Anderson) .red[raised questions about the validity] of the results.

After Duke was alerted that the PI of trial lied on his CV about being a Rhodes scholar(!), an investigation was launched that uncovered:
* data management errors 
* incorrectly designed computational models
* failure to validate gene expression tests 
* what appeared to be intentional data corruption

---

class: title-slide, center, inverse, middle
background-color: #828280

*This is not rocket science. There is computer code that evaluates the algorithm. There is data. And when you plug the data into that code, you should be able to get the answers back that you have reported. And to the extent that you canâ€™t do that, there is a problem in one or both of those items.*
<div> - Lisa McShane

---

## Levels of reproducibility

* Are the tables and figures reproducible using the code?
* Are the tables and figures reproducible using the code __with minimal user effort__?
* Does the code __actually do__ what's written in Methods?
* Does the code make clear __why__ certain decisions were made?
	* Why did you exclude this variable? 
	* Why do you choose these tuning parameters?
* Can the code be used on other data?
* Can the code be extended to do more?

???

Reproducibility is not black and white. The ideal is not easy to 
achieve, nor is it necessarily warranted on every project. This is a lifelong
process of understanding what level of reproducibility is required for 
each project. 

---

## Basic principles

* Get data in its "raw-est" form possible
* __Document__ where/when you got the data
* Be as __self-sufficient__ as possible
* __Everything via code__
	* with sufficient documentation!
* __Everything automated__
	* with sufficient documentation! 

???

The goal is end-to-end reproducibility. In many situations, 
*you* will be the person with the most data 
management experience. Reproducibility can easily be undermined
by e.g., a collaborator who is manually extracting data from an 
online data base and copy-pasting it into a .csv for you. 

Solution: Code the download process! Automate data pulls! Document
how you're doing it!

---

## Things to avoid

* Open a file to re-save as `.csv`. 
* Open a data file and manually edit. 
* Pasting results into the tables for a manuscript.
* Copy-paste-edit code into GUI
* Copy-paste-edit tables
* Copy-paste-adjust figures (with due respect to MS Paint)

???

All of these actions are problematic. Each introduces the possibility
of inconsistencies or errors and is not reproducible in the sense 
that we mean. 

---
class: title-slide, center, inverse, middle
background-color: #007dba

*If you do anything "by hand" once, you'll have to do it 100 times.*
<div> - Karl Broman

???

So true. 

---

## Tools for reproducible research

* Unix command line
* Scripting languages (e.g., `bash`, `zsh`)
* Markdown, RMarkdown, Latex, Knitr
* Version control (e.g., git and GitHub)
* GNU Make
* R packages
* Containers (e.g., Docker)

???

In this class, we'll be learning to operate almost exclusively on 
the command line. Soon, [this will be you](https://tenor.com/8IZG.gif). 

Git will make your life easier by handling version control. No more
directories with 200 copies of the same file with different dates!

RMarkdown + knitr are great tools for generating reproducible reports (or
course notes...). 

GNU Make is for automation and for documenting dependencies between 
files. 

`R` packages are a useful way to wrap up code, documentation, and dependencies
on other `R` packages. 

Containers make it possible to achieve full reproducibility rather 
painlessly, by wrapping all needed software (down to the OS!) up into a 
"container". 

Knowing some `bash` is necessary to operate at the command line and 
generally useful for scripting. `R` is becoming (or already is) a standard 
software for analysis. `python` is also becoming quite popular for analysis, 
but goes much beyond. Other languages are great, but not totally necessary.

---
class: title-slide, center, inverse, middle
background-color: #487f84

*The most important tool is the mindset, when starting, that the end product will be reproducible.*
<div> - Keith Baggerly


